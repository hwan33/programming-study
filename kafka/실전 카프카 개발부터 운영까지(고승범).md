# 실전 카프카 개발부터 운영까지

저자 : 고승범

# 1장. 카프카 개요

생략

# 2장. 카프카 환경 구성

<details>
<summary>키 페어 생성 및 권한 변경</summary>
<br>
    
- 키페어의 생성은 클라우드 콘솔에서 생성 가능
- 아래 명령어를 이용해 키 페어 권한 변경 가능

```bash
chmod 600 keypair.pem
```
</details>

<details>
<summary>키 페어 이용해서 퍼블릭 IP 주소 접근</summary>
    <br>

- 아래 두 명령어 모두 가능

```bash
ssh -i keypair.pem -l ec2-user 13.125.209.60
ssh -i keypair.pem ec2-user@13.125.209.60
```
</details>

<details>
<summary>/etc/hosts 수정을 통한 서버 IP와 호스트네임 매핑</summary>
    <br>

- /etc/hosts 파일 예제
    - ip는 사설 ip
    - 모든 인스턴스에 설정

> 172.31.3.209 peter-ansible01.foo.bar peter-ansible01
> ... // 생략

- ping test 명령어

```bash
ping -c 2 peter-zk01.foo.bar
```
</details>

<details>
<summary>앤서블 설치 및 기타 환경 설정</summary>
    <br>

- 앤서블 설치
```bash
sudo amazon-linux-extras install -y ansible2
```

- git 설치 및 책에서 제공하는 git clone

```bash
sudo yum install -y git
git clone https://github.com/onlybooks/kafka2
```

- 키 페어 인스턴스에 복사
    - 로컬 terminal에서

```bash
scp -i keypair.pem keypair.pem ec2-user@13.125.20.117:~
```

- 권한 변경 및 키 등록

```bash
chmod 600 keypair.pem
ssh-agent bash
ssh-add keypair.pem
```

</details>

<details>
<summary>ssh 공개 키를 생성해 사용하는 방식</summary>
    <br>

- 키를 메모리에 저장해두고 사용하는 방식은 배포 서버 재접속때 설정이 초기화되는 문제 발생
- 아래와 같은 방식으로 ssh 공개 키를 생성해 사용하는 방식도 존재한다

1. 배포 서버에서 공개 키 생성
2. 공개 키 내용을 접속하고자 하는 서버에 복사
3. 배포 서버에서 다른 서버로 비밀번호 없이 접속

```bash
ssh-keygen // 이후 무한 엔터
cat /home/ec2-user/.ssh/id_rsa.pub // 확인 후 복사
```

- 이후 각각의 서버에 로그인하여 아래 명령어 실행

```bash
vi /home/ec2-user/.ssh/authorized_keys
chmod 600 .ssh/authorized_keys
```

</details>

<details>
<summary>주키퍼 설치</summary>
    <br>

- 앤서블 명령어인 ansible-playbook을 써서 hosts 파일에 지정된 zookeeper 서버에 모두 주키퍼를 설치

```bash
cd kafka2/chapter2/ansible_playbook
ansible-playbook -i hosts zookeeper.yml
```

- 각각의 주키퍼 서버에 접근해 제대로 실행되는지 확인

```bash
sudo systemctl status zookeeper-server
```

</details>

<details>
<summary>카프카 설치</summary>
    <br>

- 앤서블 명령어인 ansible-playbook을 써서 hosts 파일에 지정된 kafka 서버에 모두 카프카를 설치

```bash
cd kafka2/chapter2/ansible_playbook
ansible-playbook -i hosts kafka.yml
```

- 각각의 카프카 서버에 접근해 제대로 실행되는지 확인

```bash
sudo systemctl status kafka-server
```

</details>

<details>
<summary>토픽 생성</summary>
    <br>

- 카프카가 설치된 서버에 접속한 후 카프카에서 제공하는 도구 중 kafka-topics.sh 명령어를 이용해 토픽 생성

```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-overview01 --partitions 1 --replication-factor 3
```

</details>

<details>
<summary>컨슈머, 프로듀서 실행 및 메세지 전달</summary>
    <br>

- 컨슈머 실행

```bash
/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server peter-kafka01:9092 --topic peter-overview01
```

- 프로듀서 실행

```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-overview01
```

- 메세지 전송
    - 프로듀서 실행 이후 명령 프롬포트가 `>`로 변경되면 메세지 입력
</details>

# 3장. 카프카 기본 개념과 구조

### 3.1) 카프카 기초 다지기

- 리플리케이션 : 각 메세지를 여러 개로 복제해서 카프카 클러스터 내 브로커들에 분산시키는 동작
    - 테스트나 개발 환경 : replication factor 1
    - 운영 환경 (로그성 메세지로서 약간의 유실 허용) : replication factor 2
    - 운영 환경 (유실 허용하지 않음) : replication factor 3

- 파티션 : 하나의 토픽이 한 번에 처리할 수 있는 한계를 높이기 위해 토픽 하나를 여러 개로 나눠 병렬 처리가 가능하게 만든 것
    - 파티션 수는 초기 생성 후 언제든지 늘릴 수 있지만, 한 번 늘린 파티션 수는 절대로 줄일 수 없음을 명심하자
    - 초기에는 2 또는 4로 생성하는 것을 추천

- 세그먼트 : 브로커의 로컬 디스크에 저장되는 로그 파일 형태

<details>
<summary>세그먼트 확인</summary>
    <br>

- 카프카가 설치된 서버에 접속

```bash
cd /data/kafka-logs/
ls
cd peter-overview01-0
ls
xxd 000000000000000000.log
```

</details>

### 3.2) 카프카의 핵심 개념

- 분산 시스템 : 시스템 확장에 용이
- 페이지 캐시 : 높은 처리량
- 배치 전송 처리 : 네트워크 오버헤드 감소
- 압축 전송 : 성능 향상
    - 배치 전송과 함께 사용 시 효율 증가
    - 높은 압축률이 필요한 경우 gzip, zstd
    - 빠른 응답 속도가 필요한 경우 lz4, snappy
- 토픽, 파티션, 오프셋
    - 토픽 : 이메일 주소의 개념
    - 파티션 : 토픽의 병렬 처리를 위한 단위
    - 오프셋 : 파티션의 메시지가 저장되는 위치
- 고가용성 보장
    - 카프카에서 제공하는 리플리케이션 기능은 토픽 자체를 복제하는 것이 아니라 토픽의 파티션을 복제
    - 원본과 리플리케이션을 구분하기 위해 leader와 follower라고 부른다
- 주키퍼의 의존성
    - 주키퍼는 여러 대의 서버를 앙상블(클러스터)로 구성하고, 살아 있는 노드 수가 과반수 이상 유지된다면 지속적인 서비스가 가능한 구조
    - 주키퍼는 반드시 홀수로 구성해야 한다

### 3.3) 프로듀서의 기본 동작

![producer design](https://jashangoyal.files.wordpress.com/2019/03/producer.png?w=810)

- `ProducerRecord` : 카프카로 전송하기 위한 실제 데이터
    - 토픽과 밸류는 필수 입력
    - 파티션 값을 지정했다면 파티셔너는 아무런 동작도 하지 않게 된다
    - 파티션 값을 지정하지 않았다면 라운드 로빈 방식으로 동작
- 프로듀서가 카프카로 데이터를 배치로 전송하기 위해 `send()` 메서드 이후 레코드들을 파티션별로 잠시 모아둔다
- 프로듀서의 전송 방법은 크게 3가지 방식으로 나뉜다
    - 메세지를 보내고 확인하지 않기
        - `Future` 객체에서 `get()` 메서드를 호출하지 않음
        - 카프카 브로커에게 메세지를 전송한 후의 에러는 무시하지만, 전송 전에 에러가 발생하면 예외 처리 가능
    - 동기 전송
        - `Future` 객체에서 `get()` 메서드를 호출해 `RecordMetadata` 리턴 받음
        - 카프카로 메세지를 보내기 전과 보내는 동안 에러가 발생하면 예외가 발생
    - 비동기 전송
        - `org.apache.kafka.clients.producer.Callback` 구현체 클래스 생성 및 `onCompletion()` 메서드 오버라이드
        - `send()` 메서드에 레코드와 함께 콜백 객체를 전달
        - 빠른 전송이 가능하고, 메세지 전송이 실패해도 예외 처리 가능

### 3.4) 컨슈머의 기본 동작

- 메시지는 브로커의 로컬 디스크에 저장되어 있음
- 컨슈머는 반드시 컨슈머 그룹에 속하게 되며, 컨슈머 그룹은 각 파티션 리더에게 카프카 토픽에 저장된 메세지를 요청
- 파티션 수와 컨슈머 수는 일대일로 매핑되는 것이 이상적
- 컨슈머에서 메세지를 가져오는 방법은 크게 3가지 방식으로 나뉜다
    - 오토 커밋
        - `enable.auto.commit` 설정을 `true`로 적용
        - 많이 사용하는 방식
        - 오프셋을 주기적으로 커밋하므로 관리자가 오프셋을 따로 관리하지 않아도 된다
        - 반면, 컨슈머 종료 등이 빈번히 일어나면 일부 메세지를 못 가져오거나 중복으로 가져온다
    - 동기 가져오기
        - while 반복문 안에서 가져온 레코드 처리 이후 `consumer.commitSync()` 메서드 호출
        - 현재 배치를 통해 읽은 모든 메세지를 처리한 후, 추가 메세지를 폴링하기 전 현재의 오프셋을 동기 커밋
        - 속도는 느리지만, 메시지 손실은 거의 발생하지 않음
        - 메시지의 중복 이슈는 피할 수 없음
    - 비동기 가져오기
        - while 반복문 안에서 가져온 레코드 처리 이후 `consumer.commitAsync()` 메서드 호출
        - 현재 배치를 통해 읽은 모든 메세지를 처리한 후, 추가 메세지를 폴링하기 전 현재의 오프셋을 비동기 커밋
        - `commitAsync()`은 `commitSync()`과 달리 오프셋 커밋을 실패하더라도 재시도하지 않는다
            - 재시도로 인해 오프셋이 앞으로 당겨진만큼 메시지가 중복 처리된다
        - 비동기 커밋이 계속 실패하더라도 마지막의 비동기 커밋만 성공하면 되며, 콜백을 같이 사용해서 보완할 수 있다

# 4장. 카프카의 내부 동작 원리와 구현

### 4.1) 카프카 리플리케이션

<details>
<summary>간단한 코드 예제</summary>
<br>

- 카프카 토픽 생성
```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3
```

- 카프카 토픽 상세보기
```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test01 --describe
```

- 카프카 메세지 프로듀스
```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test01
```

- 카프카 세그먼트 파일 내용 확인
```bash
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
```
</details>

<br>

- 리플리케이션 동작 개요
    - N개의 리플리케이션이 있는 경우 N-1 까지의 브로커 장애가 발생해도 메시지 손실 없음
<br>
- 리더와 팔로워
    - 모든 읽기와 쓰기는 리더를 통해서만 가능하다
<br>
- 복제 유지와 커밋
    - 리더와 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있음
    - ISR에 속하지 않은 팔로워는 리더의 자격을 가질 수 없다
    - 팔로워는 지속적으로 리더를 따라가고, 리더는 ISR 내 모든 팔로워가 메세지를 받는지 감시한다
    - 팔로워가 지속적으로 따라오지 않으면 리더는 ISR 그룹에서 해당 팔로워를 추방
    <br>
    - ISR 내 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋 표시를 한다
    - 마지막 커밋 오프셋 위치는 하이워터마크(high water mark)라 부른다
    - 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다
    <br>
    - 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 replication-offset-checkpoint라는 파일에 마지막 커밋 오프셋 위치를 저장
    - 메세지가 프로듀스되고 커밋되면 1씩 증가됨을 확인할 수 있다

        <details>
        <summary>replication-offset-checkpoint 확인 코드</summary>

        ```bash
        cat /data/kafka-logs/replication-offset-checkpoint
        ```
        </details>

        <br>

- 딘계별 리플리케이션 동작
    - 서로의 통신을 최소화할 수 있도록 설계되어 리더의 부하를 줄였다
    <br>
    - 팔로워들의 fetch 요청으로 리더가 리플리케이션이 요청되었음은 알 수 있으나, 성공 유무는 알 수 없다
    - 래빗MQ의 트랜잭션 모드에서는 모든 미러가 메세지를 받았는지에 대한 ACK를 리더에게 리턴하므로 알 수 있지만, 카프카는 ACK 통신을 주고 받지 않는다
        - 성능을 높이기 위해 ACK 통신 제거
    
    <br>

    - 이후 새로운 메시지가 프로듀싱되어 팔로워들이 새로운 오프셋에 대한 리플리케이션을 요청하게 된다
    - 새로운 오프셋에 대한 리플리케이션 요청이 들어오면 리더는 이전 오프셋에 대한 동작은 성공했다고 인지하고 이전 오프셋에 커밋 표시를 한 후 하이워터마크를 증가시킨다
    - 이전 오프셋으로 요청하는 팔로워가 있으면 실패했음을 리더가 알 수 있다
    - 새로운 오프셋 메시지에 대한 요청을 받은 리더는 응답에 이전 오프셋이 커밋되었음을 함께 전달한다
    - 이전 오프셋이 커밋되었다는 응답을 받은 팔로워는 동일하게 커밋을 표시하고나서 새로운 메시지를 리플리케이션한다
    <br>
- 리더에포크와 복구
    - 리더에포크는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지
    - 컨트롤러에 관리되는 32비트의 숫자로 표현된다
    <br>
    - 리더와 팔로워 간 하이워터마크가 1 차이나기에 발생하는 문제를 해결한다
        1. 팔로워가 복구된 이후 자신의 하이워터마크보다 높은 메시지를 즉시 삭제해버리고, fetch 요청을 했을 때 리더가 다운된 경우
        2. 리더 팔로워 모두 다운되고 팔로워가 먼저 복구되어 리더로 승격되었고, 새로운 메시지를 받은 이후 기존의 리더가 복구된 경우
    - 1번 문제의 경우, 팔로워가 복구된 이후 자신의 하이워터마크보다 높은 메시지를 삭제하지 않고 리더에게 리더에포크를 요청하여 해결
    - 2번 문제의 경우, 새로운 리더는 자신이 팔로워일 때의 하이워터마크와 새로운 리더일 때의 라이워터마크를 알고 있으며, 리더에포크 요청이 오면 자신이 팔로워일 때의 하이워터마크를 보내서 이전 리더에만 있던 메시지를 없애고, 자신에게만 추가된 메시지를 리플리케이션한다

    <details>
    <summary>리더에포크 실습 코드</summary>

    ```bash
    # 토픽 생성
    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test02 --partitions 1 --replication-factor 3

    # 토픽 상세보기를 통해 리더가 어느 브로커인지 확인
    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test02 --describe

    # 리더 브로커 접속 이후 리더에포크 상태 확인
    cat /data/kafka-logs/peter-test02-0/leader-epoch-checkpoint

    # 메세지 프로듀스 -> 리더에포크 변화 없음
    /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka02:9092 --topic peter-test02

    # 리더 강제 종료 -> 이후 새로운 리더 브로커로 접속
    sudo systemctl stop kafka-server
    sudo systemctl status kafka-server

    # 새로운 리더의 리더에포크 상태 확인
    cat /data/kafka-logs/peter-test02-0/leader-epoch-checkpoint
    ```
    </details>

### 4.2) 컨트롤러

- 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하며, 파티션의 ISR 리스트 중에서 리더를 선출
    - ISR 리스트 정보는 가용성 보장을 위해 주키퍼에 저장
    - 브로커의 실패가 감지되면 즉시 ISR 리스트 중 하나를 새로운 파티션 리더로 선출하고, 주키퍼에 기록하며, 변경 사항을 모든 브로커에게 전달한다
- 클라이언트에 설정되어 있는 재시도 숫자만큼 재시도를 하게 되므로 새로운 리더 선출 작업이 빠르게 이뤄져야 한다

<details>
<summary>예기치 않은 상황에서의 리더 선출 과정</summary>

1. 파티션의 리더가 있는 브로커가 다운
2. 주키퍼는 브로커와 연결이 끊어진 후, 파티션의 ISR에서 변화를 감지
3. 컨트롤러는 주키퍼 워치를 통해 파티션 변화를 감지하고, 새로운 리더를 선출
4. 컨트롤러는 해당 파티션의 새로운 리더에 대한 정보를 주키퍼에 기록
5. 갱신된 정보는 활성화 상태인 모든 브로커에 전달
</details>
<details>
<summary>자연스러운 종료 상황에서의 리더 선출 과정</summary>

1. 관리자가 브로커 종료 명령어를 실행하고, `SIG_TERM` 신호가 브로커에 전달
2. `SIG_TERM` 신호를 받은 브로커는 컨트롤러에게 알림
3. 컨트롤러는 리더 선출 작업을 진행하고, 해당 정보를 주키퍼에 기록
4. 컨트롤러는 새로운 리더 정보를 다른 브로커에 전송
5. 컨트롤러는 종료 요청을 보낸 브로커에게 정상 종료한다는 응답을 보낸다
6. 응답을 받은 브로커는 캐시에 있는 내용을 디스크에 저장하고 종료한다
</details>
<br>

- 제어된 종료와 급작스러운 종료의 가장 큰 차이는 다운타임이다
    - 제어된 종료를 사용하면 카프카 내부적으로 파티션들의 다운타임을 최소화할 수 있다
    - 브로커가 종료되기 전에 리더 선출 작업을 진행하기 때문이다
    - 제어된 종료에서는 모든 로그를 디스크에 동기화한 후 종료되므로, 재시작할 때 로그 복구 시간이 짧다
    - `controlled.shutdown.enable = true` 설정을 통해 제어된 종료를 사용 가능하다

        <details>
        <summary>현재 브로커 설정 확인</summary>

        ```bash
        /usr/local/kafka/bin/kafka-configs.sh --bootstrap-server peter-kafka01:9092 --broker 1 --describe --all
        ```
        </details>

### 4.3) 로그(로그 세그먼트)

- 카프카의 토픽으로 들어오는 메시지는 세그먼트라는 파일에 저장된다
- 로그 세그먼트에는 내용 뿐만 아니라 메시지의 키, 밸류, 오프셋, 메시지 크기 같은 정보가 함께 저장되며, 브로커의 로컬 디스크에 보관된다
- 최대 크기는 1GB가 기본값으로 설정되어 있므여, 1GB보다 커지는 경우 기본적으로 롤링 전략을 사용
    - 1GB에 도달하면 해당 세그먼트 파일을 close하고, 새로운 로그 세그먼트를 생성하는 방식
- 로그 세그먼트를 관리하는 방법은 아래 두가지로 크게 나뉜다
    - 로그 세그먼트 삭제
    - 로그 컴팩션
<br>

- 로그 세그먼트 삭제
    - 브로커의 설정 파일인 `server.properties`에서 `log.cleanup.policy`가 `delete`로 명시되어야 한다
    - 로그 세그먼트 파일명 생성 규칙
        - 오프셋 시작 번호를 이용해 파일 이름 생성
    - `retention.ms` 기본 값은 7일
    - `retention.bytes` 옵션을 이용해 시간이 아닌 지정된 크기 기준으로 로그 세그먼트 삭제 가능

        <details>
        <summary>실습 코드</summary>

        ```bash
        # 실습용 토픽 생성
        /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test03 --partitions 1 --replication-factor 3

        # 메세지 전송
        /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03

        # 토픽의 처음부터 메시지 가져오기
        /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --from-beginning

        # 메시지 삭제하기 옵션 추가
        # retention.ms = 0이란 로그 세그먼트 보관 시간이 해당 숫자보다 크면 세그먼트를 삭제한다는 명령
        /usr/local/kafka/bin/kafka-configs.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --add-config retention.ms=0 --alter

        # 설정 내용 확인
        /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --describe

        # 로그 세그먼트 상태 확인
        # 로그 세그먼트 삭제 작업의 일정 주기 기본값이 5분 주기이므로, 5분 뒤에 확인하면 00..000으로 시작하는 파일이 모두 삭제되고, 00..001으로 시작하는 파일이 새로 생성됨을 확인할 수 있다
        ls /data/kafka-logs/peter-test03-0/

        # retention.ms 옵션 삭제
        /usr/local/kafka/bin/kafka-configs.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --delete-config retention.ms --alter

        # 설정 내용 확인
        /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --describe
        ```
        </details>
<br>

- 로그 컴팩션
    - 현재 활성화된 세그먼트를 제외하고 나머지 세그먼트를 대상으로 실행된다
    - 카프카에서 로그 세그먼트를 컴팩션하면 메시지의 키값을 기준으로 마지막의 데이터만 보관한다
    - 대표 예제는 `__consumer_offset` 토픽
        - 컨슈머 그룹의 정보를 저장하는 토픽
        - 해당 컨슈머 그룹이 어디까지 읽었는 지를 나타내는 오프셋 커밋 정보
        - 키(컨슈머 그룹명, 토픽명)와 밸류(오프셋 커밋 정보) 형태로 메시지가 저장
    - 로그 컴팩션 기능을 사용하려면, 프로듀서가 카프카로 메시지를 전송할 때 필수 값인 밸류 뿐만 아니라 키도 반드시 전송해야 한다
    <br>
    - 장점 : 빠른 장애 복구가 가능하다
        - 전체 로그를 복구하지 않고, 최신의 상태만 복구 가능하므로
        - 모든 토픽이 아닌 최종값만 필요한 워크로드에 적용하는 것이 바람직
    - 로그 컴팩션 작업 시에 브로커의 과도한 입출력 부하가 발생할 수 있으니 브로커의 리소스 모니터링과 병행하는 것을 권장

# 5장. 프로듀서의 내부 동작 원리와 구현

### 5.1) 파티셔너

- 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보내야 할지를 결정해야 하는데, 이때 파티셔너가 사용된다
    - 프로듀서가 파티션을 결정하는 알고리즘은 기본적으로 메시지의 키를 해싱한다
    - 토픽의 파티션을 늘리게 되면 해싱 테이블이 변경되면서 동일한 키가 다른 파티션으로 전송될 수 있다
        - 따라서 메시지의 키를 이용해 메시지를 전송하는 경우에는 되도록 파티션 수를 변경하지 않는 것을 권장한다
    - 프로듀서의 메시지 중 레코드의 키값은 필수가 아니므로 지정하지 않을 수 있다
        - 키값은 null, 기본값인 라운드 로빈 알고리즘을 이용해 파티션으로 랜덤 발송한다

1. 라운드 로빈 전략
    - 파티셔너를 거친 후의 레코드들은 배치 처리를 위해 프로듀서의 버퍼 메모리 영역에서 잠시 대기 후 카프카로 전송된다
        - 잠시 대기하는 과정에서 라운드 로빈 전략은 비효율적이다
        - 각 파티션별로 배치 전송을 위해 필요한 레코드 수에 따라 무한 대기할 수 있다
        - 프로듀서의 옵션 조정으로 특정 시간 초과 시 레코드 전송을 가능하게 할 수 있으나 배치와 압축 효과가 없어져 비효율적이다
    - 위의 문제들을 보완하기 위해 스티키 파티셔닝 전략이 도입되었다
<br>

2. 스티키 파티셔닝 전략
    - 하나의 파티션에 레코드 수를 먼저 채워서 카프카로 빠르게 배치 전송하는 전략
    - 기본 설정에 비해 약 30% 이상 지연시간이 감소하고 프로듀서의 CPU 사용률이 줄어듦
    - 카프카로 전송하는 메시지의 순서가 중요하지 않은 경우라면 스티키 파티셔닝 전략을 적용하기를 권장

### 5.2) 프로듀서의 배치

- 프로듀서는 배치 전송을 위해 아래 옵션을 제공한다
    - `buffer.memory`
        - 카프카로 메시지를 전송하기 위해 담아두는 프로듀서의 버퍼 메모리 옵션
        - 프로듀서의 전체 버퍼 크기이며, 기본값은 32MB
    - `batch.size`
        - 배치 전송을 위해 메시지들을 묶는 단위를 설정하는 배치 크기 옵션
        - 각 파티션 안에서 배치 전송을 위해 필요한 레코드들의 크기이며, 기본값은 16KB
    - `linger.ms`
        - 배치 전송을 위해 버퍼 메모리에서 대기하는 메시지들의 최대 대기시간 옵션
        - 단위는 ms이며 기본값은 0
        - 기본값 0으로 설정하면, 배치 전송이 아니라 즉시 전송된다

- 배치 전송은 불필요한 I/O와 카프카의 요청 수를 줄일 수 있어 매우 효율적이다
- 목적에 따라 선택해야 한다
    - 처리량을 높이려면 `batch.size`와 `linger.ms`의 값을 크게 설정
    - 지연 없는 전송이 목표라면 `batch.size`와 `linger.ms`의 값을 작게 설정

- 높은 처리량을 목표로 하는 경우 주의점
    - `buffer.memory` 크기는 반드시 `batch.size`보다 커야 한다

### 5.3) 중복 없는 전송

- 메시지 시스템의 메시지 전송 방식
    1. 적어도 한 번 전송 (at least once)
    2. 최대 한 번 전송 (at most once)
        - ACK 응답 안해도 됨
        - 일부 메시지가 손실되더라도 높은 처리량을 필요로 하는 대량의 로그 수집이나 IoT 환경에서 사용됨
    3. 정확히 한 번 전송 (exactly once)
<br>

<details>
<summary>적어도 한 번 전송 예상 시나리오</summary>
1. 프로듀서가 브로커의 특정 토픽으로 메시지 A를 전송
2. 브로커는 메시지 A를 기록하고, ACK 응답
3. 브로커의 ACK를 받은 프로듀서는 다음 메시지 B를 전송
4. 브로커는 메시지 B를 기록하고, ACK를 응답하려고 했으나 문제가 발생하여 프로듀서가 B에 대한 ACK 응답을 받지  못함
5. ACK를 받지 못한 프로듀서는 브로커가 메시지 B를 받지 못했다고 판단해 재전송
</details>
<details>
<summary>최대 한 번 전송 예상 시나리오</summary>
1. 프로듀서가 브로커의 특정 토픽으로 메시지 A를 전송
2. 브로커는 메시지 A를 기록하고, ACK 응답
3. 브로커의 ACK를 받은 프로듀서는 다음 메시지 B를 전송
4. 브로커는 메시지 B를 기록하지 못하고, 잘 받았다는 ACK 응답 전송 실패
5. 프로듀서는 브로커가 메시지 B를 받았다고 가정하고, 메시지 C를 전송
</details>
<br>

- 중복 없는 전송
    - 프로듀서의 역할은 적어도 한 번 전송 방식과 동일
    - 브로커가 PID 및 메시지 번호를 비교하여 중복 메시지를 받은 경우 처리 없이 ACK만 보낸다
    - 프로듀서는 고유한 PID를 할당 받음
    - 메시지 번호는 0부터 순차적으로 증가
    - PID와 메시지 번호는 브로커의 메모리에 유지되고, 리플리케이션 로그에도 저장됨
    - 오버헤드가 존재하지만 단순한 숫자 필드만 추가하는 방식이므로 기존 대비 최대 약 20% 정도만 성능이 감소
<details>
<summary>중복 없는 전송 예상 시나리오</summary>
1. 프로듀서가 브로커의 특정 토픽으로 메시지 A를 전송, 이때 PID 0과 메시지 번호 0을 헤더에 포함해 함께 전송
2. 브로커는 메시지 A를 저장하고 PID와 메시지 번호를 메모리에 기록, 이후 ACK 응답
3. 브로커의 ACK를 받은 프로듀서는 다음 메시지 B를 전송, PID는 동일하게 0이고 메시지 번호는 1
4. 브로커는 메시지 B를 저장하고, PID와 메시지 번호를 메모리에 기록, 하지만 ACK 응답 전송 실패
5. ACK를 받지 못한 프로듀서는 브로커가 메시지 B를 받지 못했다고 판단해 메시지 B 재전송
</details>

<details>
<summary>중복 없는 전송 실습 코드</summary>

```bash
# 실습용 토픽 생성
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test04 --partitions 1 --replication-factor 3

# 별도의 프로듀서 설정 파일 생성
vi /home/ec2-user/producer.config

enable.idempotence=true
max.in.flight.requests.per.connection=5
retries=5

# 프로듀서 설정 파일 로드
# 아래 커맨드를 입력하면 "org.apache.kafka.common.config.ConfigException: Must set acks to all in order to use the idempotent producer. Otherwise we cannot guarantee idempotence." 에러가 발생한다
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test04 --producer.config /home/ec2-user/producer.config

# acks=all 옵션 추가
vi /home/ec2-user/producer.config

enable.idempotence=true
max.in.flight.requests.per.connection=5
retries=5
acks=all

# 콘솔 프로듀서 실행
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test04 --producer.config /home/ec2-user/producer.config

# 메시지 전송 이후 snapshot 파일이 있는지 확인
cd /data/kafka-logs/peter-test04-0/
ls

# snapshot이 없다면 리더 브로커 종료 후 재실행
# 있다면 dump로 snapshot 파일 확인
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test04-0/00000000000000000001.snapshot
```
</details>

### 5.4) 정확히 한번 전송

- 위의 내용에서와 같이 카프카에서는 멱등성 옵션을 이용해 중복 없는 전송을 할 수 있다
- 카프카에서 정확히 한 번 전송은 트랜잭션 같은 전체적인 프로세스 처리를 의미하며, 중복 없는 전송은 일부 기능이라 할 수 있다
- 전체적인 프로세스를 관리하기 위해 카프카에서는 정확히 한 번 처리를 담당하는 별도의 프로세스가 존재

<br>

- 정확히 한 번 방식으로 메시지를 전송하면, 프로듀서가 보내는 메시지들은 atomic 처리되어 전송에 성공하거나 실패한다
- 프로듀서에 의해 전송된 메시지를 관리하며, 커밋 또는 중단 등을 표시하는 트랜잭션 코디네이터라는 것이 서버 측에 존재한다
    - PID와 `transaction.id`를 매핑하고 해당 트랜잭션 전체를 관리
- 카프카에서는 트랜잭션 로그를 카프카의 내부 토픽인 `__transaction_state`에 저장한다
    - 내부 토픽이므로 파티션 수와 리플리케이션 팩터 수가 존재
    - 기본값
        - `transaction.state.log.num.partitions=50`
        - `transaction.state.log.replication.factor=3`
- 프로듀서는 트랜잭션 관련 정보를 트랜잭션 코디네이터에게 알리고, 모든 정보의 로그는 코디네이터가 직접 기록한다
- 메시지가 정상 커밋된 것인지 실패한 것인지 식별하기 위해 컨트롤 메시지라고 불리는 특별한 타입의 메시지가 추가로 사용된다
    - 페이로드에 애플리케이션 데이터(메시지의 밸류)를 포함하지 않으며, 애플리케이션들에게 노출되지 않는다
    - 오직 브로커와 클라이언트 통신에서만 사용된다

<br>

- `TRANSACTIONAL_ID_CONFIG` 옵션은 실행하는 프로듀서 프로세스마다 고유한 아이디로 설정해야 한다
    - 중복 없는 전송과 가장 큰 차이점

<details>
<summary>단계별 동작</summary>

1. 트랜잭션 코디네이터 찾기
    - 프로듀서는 브로커에서 `FindCoordinatorRequest`를 보내서 브로커에 있는 트랜잭션 코디네이터의 위치를 찾는다
    - 만약 트랜잭션 코디네이터가 존재하지 않는다면 신규 트랜잭션 코디네이터가 생성된다
    - `__transaction_state` 토픽의 파티션 번호는 `transaction.id`를 기반으로 해시하여 결정되고, 이 파티션의 리더가 있는 브로커가 트랜잭션 코디네이터의 브로커로 최종 선정된다
2. 프로듀서 초기화
    - 프로듀서는 `initTransaction()` 메소드를 이용해 트랜잭션 전송을 위한 `InitPidRequest`를 트랜잭션 코디네이터로 보낸다
    - TID가 설정된 경우에는 함께 전송된다
    - 트랜잭션 코디네이터는 TID, PID를 매핑하고 해당 정보를 트랜잭션 로그에 기록한다
    - PID epoch를 한 단계 올린다 (이전의 동일한 PID와 이전 에포크에 대한 쓰기 요청 무시)
3. 트랜잭션 시작
    - 프로듀서는 `beginTransaction()` 메소드를 이용해 트랜잭션을 시작한다
    - 프로듀서는 내부적으로 이를 기록하지만, 트랜잭션 코디네이터 관점에서는 첫 번째 레코드가 전송될 때까지 트랜잭션이 시작된 것은 아니다
4. 트랜잭션 상태 추가
    - 트랜잭션 코디네이터는 전체 트랜잭션을 관리하므로 각 트랜잭션 상태를 기록해야 한다
    - TID, 파티션 정보가 트랜잭션 로그에 기록되며, 트랜잭션의 현재 상태를 `Ongoing`으로 표시한다
    - 만약 트랜잭션 로그에 추가되는 첫 번째 파티션이라면, 트랜잭션 코디네이터는 해당 트랜잭션에 대한 타이머를 시작하고, 기본값으로 1분 동안 트랜잭션 상태에 대한 업데이트가 없다면 해당 트랜잭션은 실패로 처리된다
5. 메시지 전송
    - 프로듀서는 대상 토픽의 파티션으로 PID, 에포크, 시퀀스 번호와 함께 메시지를 전송한다
    - 트랜잭션 코디네이터가 있는 브로커와 프로듀서가 전송하는 메시지를 받는 브로커는 서로 다르다
6. 트랜잭션 종료 요청
    - 메시지 전송을 완료한 프로듀서는 `commitTransaction()` 메소드 또는 `abortTransaction()` 메소드 중 하나를 반드시 호출해야 하며, 이를 통해 트랜잭션이 완료됨을 트랜잭션 코디네이터에게 알린다
    - 트랜잭션 코디네이터는 두 단계의 커밋 과정을 시작하며, 첫 번째 단계로 트랜잭션 로그에 해당 트랜잭션에 대한 `PrepareCommit` 또는 `PrepareAbort`를 기록한다
7. 사용자 토픽에 표시 요청
    - 트랜잭션 코디네이터는 두 번째 단계로서 트랜잭션 로그에 기록된 토픽의 파티션에 트랜잭션 커밋 표시를 기록한다
    - 여기서 기록하는 메시지가 컨트롤 메시지이다
8. 트랜잭션 완료
    - 트랜잭션 코디네이터는 완료됨(Committed)이라고 트랜잭션 로그에 기록하고, 프로듀서에게 해당 트랜잭션이 완료됨을 알린다
    - 트랜잭션을 이용하는 컨슈머는 `read_committed` 설정을 하면 트랜잭션에 성공한 메시지들만 읽을 수 있다
</details>

<details>
<summary>정확히 한 번 전송 실습 코드</summary>

```bash
# 실습용 토픽 생성
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test05 --partitions 1 --replication-factor 3

# 앤서블에서 자바 설치
sudo yum install -y java 1.8.0-openjdk java-1.8.0-openjdk-devel
java -version

# 앤서블에서 트랜잭션 프로듀서 자바 코드를 이용해 메시지 전송
# "Message sent successfully" 로그를 확인할 수 있다
cd kafka2/chapter5
java -jar ExactlyOnceProducer.jar

# 브로커에서 토픽 리스트 확인
# "__transaction_state" 라는 토릭이 생김
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092  --list

# 해당 토픽을 보기 위해 컨슈머 옵션 추가
vi /home/ec2-user/consumer.config
exclude.internal.topics=false

# __transaction_state 토픽 내용 읽기
# 트랜잭션 초기화, 상태 표시 및 메시지 전송, 트랜잭션 종료 요청, 트랜잭션 완료 총 4개의 로그가 보인다
# 1. 트랜잭션 초기화 : PID 48000, state = Empty, topicPartitions = Set()
# 2. 상태 표시 및 메시지 전송 : state = Ongoing, topicPartitions = Set(peter-test05-0)
# 3. 트랜잭션 종료 요청 : state = PrepareCommit
#   - 트랜잭션 프로듀서는 peter-test05 토픽의 0번 파티션으로 메시지 전송 완료했으며, 트랜잭션을 종료하기 위해 트랜잭션 코디네이터에 트랜잭션 종료 요청을 보낸 상태
#   - 트랜잭션 코디네이터는 두 단계 커밋 과정 중 1단계 커밋까지 완료
# 4. 트랜잭션 완료 : state = CompleteCommit
#   - 트랜잭션 프로듀서가 peter-test05 토픽으로 보낸 메시지에 대한 트랜잭션 단계가 최종적으로 완료
#   - 트랜잭션 코디네이터는 두 단계 커밋 과정 중 2단계 커밋까지 완료
/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server peter-kafka01:9092 --topic __transaction_state --consumer.config /home/ec2-user/consumer.config --formatter "kafka.coordinator.transaction.TransactionLog\$TransactionLogMessageFormatter" --from-beginning

# peter-test05 토픽의 로그 파일 확인
# 트랜잭션 프로듀서를 이용해 하나의 메시지만 전송했는데, 세그먼트 파일에는 오프셋 0, 1이 기록되어 있다
# 추가된 1번 오프셋은 정확히 한 번 단계별 동작 중 사용자 토픽에 트랜잭션 완료 유무 표시와 관련된 내용이다
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test05-0/00000000000000000000.log
```
</details>

# 6장. 컨슈머의 내부 동작 원리와 구현

### 6.1) 컨슈머 오프셋 관리

- 오프셋 : 메시지의 위치, 숫자 형태
- 컨슈머 그룹은 오프셋 정보를 가장 안전한 저장소인 토픽 `__consumer_offsets`에 저장
    - 저장되는 오프셋값은 컨슈머가 다음으로 읽어야 할 위치

<details>
<summary>컨슈머의 기본 동작</summary>

1. 컨슈머는 지정된 토픽의 메시지를 읽고, 위치 정보를 `__consumer_offsets`에 기록
2. 컨슈머 변경 발생 시 `__consumer_offsets` 토픽에 기록된 정보로 해당 컨슈머가 어느 위치까지 읽었는지  추적한다
</details>

<br>

- `__consumer_offsets` 설정값
    - `offsets.topic.num.partitions` : 기본값 50
    - `offsets.topic.replication.factor` : 기본값 3

### 6.2) 그룹 코디네이터

- 컨슈머들은 서로 자신의 정보를 공유하면서 하나의 공동체로 동작하며, 언제든지 그룹을 떠날 수 있으며 새로운 컨슈머가 합류할 수 있다
- 컨슈머 리밸런싱 : 컨슈머 그룹에서 각 컨슈머들에게 작업을 균등하게 분해하는 동작
- 그룹 코디네이터 : 컨슈머 그룹 관리를 위한 별도의 코디네이터
    - 목적 : 컨슈머 그룹이 구독한 토픽의 파티션들과 그룹의 멤버들을 트래킹하며, 변화가 생기면 리밸런싱
    - 컨슈머 그룹별로 존재하며, 브로커 중 하나에 위치한다

<details>
<summary>그룹 코디네이터 동작 방식</summary>

1. 컨슈머는 `bootstrap.brokers` 리스트에 있는 브로커에게 초기 커넥션 연결 요청을 보낸다
2. 브로커는 그룹 코디네이터를 생성하고 응답을 보낸다. 컨슈머 그룹의 첫 번째 컨슈머가 등록될 때까지 아무 작업도 없다
3. 그룹 코디네이터는 `group.initial.rebalance.delay.ms`의 시간 동안 컨슈머의 요청을 기다림
4. 컨슈머 등록 요청을 그룹 코디네이터에게 보낸다. 첫 요청을 보낸 컨슈머가 그룹의 리더가 된다
5. 그룹 코디네이터는 해당 컨슈머 그룹이 구독하는 토픽 파티션 리스트 등의 응답을 보낸다
6. 리더 컨슈머는 컨슈머 파티션 할당 전략에 따라 그룹 내 컨슈머들에게 파티션을 할당한 뒤 그룹 코디네이터에 전달한다
7. 그룹 코디네이터는 해당 정보를 캐시하고 컨슈머들에게 성공을 알린다
8. 각 컨슈머들은 각자 지정된 토픽 파티션으로부터 메시지를 가져온다
</details>

<br>

- 컨슈머의 `join` 또는 `leave` 요청으로 컨슈머 그룹의 변화가 처리된다
- 변경 감지를 위해 그룹 코디네이터와 컨슈머들은 서로 heartbeat를 주고 받는다
    - 컨슈머가 잘 살아 있는지, 잘 동작하는지를 확인한다
    - `leave` 요청을 못보내도 상관 없다
- 상태 뿐만 아니라 정상 동작 여부를 `poll()`으로 확인하기도 한다

<br>

- 컨슈머 리밸런싱 동작은 비용이 크므로 가급적 자주 발생하지 않도록 주의
    - 컨슈머 다운을 더 빠르게 감지하도록 조정 : 빈번한 리밸런싱 발생
    - 컨슈머 다운을 더 느리게 감지하도록 조정 : 그 시간만큼 해당 파티션의 메시지를 읽지 못함

### 6.3) 스태틱 멤버십

- 일반적으로 컨슈머가 재시작되면 동일한 컨슈머임에도 각 컨슈머를 식별하기 위한 엔티티 ID가 새로 부여되어, 컨슈머 그룹의 리밸런싱이 발생
    - 하나의 컨슈머만 수정되어도 전체 컨슈머에 대해서 리밸런싱이 일어난다
- 이러한 불필요한 리밸런싱을 방어하기 위해 스태틱 멤버십 개념 도입
    - 컨슈머마다 인식할 수 있는 ID를 적용해 재시작해도 기존 구성원임을 인식 가능
    - 스태택 멤버십 기능이 적용된 컨슈머는 그룹에서 떠날 때 그룹 코디네이터에 알리지 않아 리밸런싱 발생하지 않음

<br>

- 스태틱 멤버십을 적용하기 위한 옵션
    - 카프카 버전 2.3이상
    - `group.instance.id` 설정 : 컨슈머 인스턴스별로 고유한 값
    - `session.timeout.ms`를 기본값보다는 큰 값으로 조정 : 재시작 시간보다 큰 값

<br>

<details>
<summary>스태틱 멤버십 실습 코드 - 환경 준비</summary>

```bash
# 파이썬 가상 환경 이용을 위한 파이썬 모듈 설치
# 모든 브로커에 각각 실행
sudo yum -y install python3
python3 -m venv venv6
source venv6/bin/activate
pip install confluent-kafka

# 토픽 생성
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test06 --partitions 3 --replication-factor 3

# 깃 설치, 클론
# 모든 브로커에 각각 실행
sudo yum -y install git
git clone http://github.com/onlybooks/kafka2
cd kafka2/chapter6/
```
</details>
<details>
<summary>스태틱 멤버십 실습 코드 - 표준 컨슈머를 사용한 경우</summary>

```bash
# 모든 브로커에 각각 실행
python consumer_standard.py

# 파이썬 프로듀서를 이용해 메시지 전송
# 브로커 한 대에서만 실행
# 새로운 터미널에서 파이썬 가상 환경 진입해야 한다
python producer.py

# 컨슈머 그룹 상태 확인
# 파티션 3개가 각각 컨슈머 3개와 매핑된 상태
/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server peter-kafka01:9092 --group peter-consumer01 --describe

# peter-kafka01의 컨슈머 프로세스 종료 이후 컨슈머 그룹 상태 확인
# 내부적으로 리밸런싱 발생
/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server peter-kafka01:9092 --group peter-consumer01 --describe
```
</details>
<details>
<summary>스태틱 멤버십 실습 코드 - 스태틱 멤버십을 적용한 컨슈머를 사용한 경우</summary>

```bash
# 모든 브로커에 각각 실행
# 스태틱 멤버십 컨슈머 실행과 동시에 프로듀서를 통해 보냈던 메시지를 모두 읽어온다
python consumer_static.py

# 컨슈머 그룹 상세보기
# CONSUMER-ID 부분이 'consumer-호스트네임-고유 문자열' 조합으로 생성됨
# 파티션 3개가 각각 컨슈머 3개와 매핑된 상태
/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server peter-kafka01:9092 --group peter-consumer02 --describe

# peter-kafka01의 컨슈머 프로세스 종료 이후 컨슈머 그룹 상태 확인
# 내부적으로 리밸런싱 발생하지 않음
# 30초 지난 후 리밸런싱 발생
/usr/local/kafka/bin/kafka-consumer-groups.sh --bootstrap-server peter-kafka01:9092 --group peter-consumer02 --describe
```
</details>