# 실전 카프카 개발부터 운영까지

저자 : 고승범

# 1장. 카프카 개요

생략

# 2장. 카프카 환경 구성

<details>
<summary>키 페어 생성 및 권한 변경</summary>
<br>
    
- 키페어의 생성은 클라우드 콘솔에서 생성 가능
- 아래 명령어를 이용해 키 페어 권한 변경 가능

```bash
chmod 600 keypair.pem
```
</details>

<details>
<summary>키 페어 이용해서 퍼블릭 IP 주소 접근</summary>
    <br>

- 아래 두 명령어 모두 가능

```bash
ssh -i keypair.pem -l ec2-user 13.125.209.60
ssh -i keypair.pem ec2-user@13.125.209.60
```
</details>

<details>
<summary>/etc/hosts 수정을 통한 서버 IP와 호스트네임 매핑</summary>
    <br>

- /etc/hosts 파일 예제
    - ip는 사설 ip
    - 모든 인스턴스에 설정

> 172.31.3.209 peter-ansible01.foo.bar peter-ansible01
> ... // 생략

- ping test 명령어

```bash
ping -c 2 peter-zk01.foo.bar
```
</details>

<details>
<summary>앤서블 설치 및 기타 환경 설정</summary>
    <br>

- 앤서블 설치
```bash
sudo amazon-linux-extras install -y ansible2
```

- git 설치 및 책에서 제공하는 git clone

```bash
sudo yum install -y git
git clone https://github.com/onlybooks/kafka2
```

- 키 페어 인스턴스에 복사
    - 로컬 terminal에서

```bash
scp -i keypair.pem keypair.pem ec2-user@13.125.20.117:~
```

- 권한 변경 및 키 등록

```bash
chmod 600 keypair.pem
ssh-agent bash
ssh-add keypair.pem
```

</details>

<details>
<summary>ssh 공개 키를 생성해 사용하는 방식</summary>
    <br>

- 키를 메모리에 저장해두고 사용하는 방식은 배포 서버 재접속때 설정이 초기화되는 문제 발생
- 아래와 같은 방식으로 ssh 공개 키를 생성해 사용하는 방식도 존재한다

1. 배포 서버에서 공개 키 생성
2. 공개 키 내용을 접속하고자 하는 서버에 복사
3. 배포 서버에서 다른 서버로 비밀번호 없이 접속

```bash
ssh-keygen // 이후 무한 엔터
cat /home/ec2-user/.ssh/id_rsa.pub // 확인 후 복사
```

- 이후 각각의 서버에 로그인하여 아래 명령어 실행

```bash
vi /home/ec2-user/.ssh/authorized_keys
chmod 600 .ssh/authorized_keys
```

</details>

<details>
<summary>주키퍼 설치</summary>
    <br>

- 앤서블 명령어인 ansible-playbook을 써서 hosts 파일에 지정된 zookeeper 서버에 모두 주키퍼를 설치

```bash
cd kafka2/chapter2/ansible_playbook
ansible-playbook -i hosts zookeeper.yml
```

- 각각의 주키퍼 서버에 접근해 제대로 실행되는지 확인

```bash
sudo systemctl status zookeeper-server
```

</details>

<details>
<summary>카프카 설치</summary>
    <br>

- 앤서블 명령어인 ansible-playbook을 써서 hosts 파일에 지정된 kafka 서버에 모두 카프카를 설치

```bash
cd kafka2/chapter2/ansible_playbook
ansible-playbook -i hosts kafka.yml
```

- 각각의 카프카 서버에 접근해 제대로 실행되는지 확인

```bash
sudo systemctl status kafka-server
```

</details>

<details>
<summary>토픽 생성</summary>
    <br>

- 카프카가 설치된 서버에 접속한 후 카프카에서 제공하는 도구 중 kafka-topics.sh 명령어를 이용해 토픽 생성

```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-overview01 --partitions 1 --replication-factor 3
```

</details>

<details>
<summary>컨슈머, 프로듀서 실행 및 메세지 전달</summary>
    <br>

- 컨슈머 실행

```bash
/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server peter-kafka01:9092 --topic peter-overview01
```

- 프로듀서 실행

```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-overview01
```

- 메세지 전송
    - 프로듀서 실행 이후 명령 프롬포트가 `>`로 변경되면 메세지 입력
</details>

# 3장. 카프카 기본 개념과 구조

### 3.1) 카프카 기초 다지기

- 리플리케이션 : 각 메세지를 여러 개로 복제해서 카프카 클러스터 내 브로커들에 분산시키는 동작
    - 테스트나 개발 환경 : replication factor 1
    - 운영 환경 (로그성 메세지로서 약간의 유실 허용) : replication factor 2
    - 운영 환경 (유실 허용하지 않음) : replication factor 3

- 파티션 : 하나의 토픽이 한 번에 처리할 수 있는 한계를 높이기 위해 토픽 하나를 여러 개로 나눠 병렬 처리가 가능하게 만든 것
    - 파티션 수는 초기 생성 후 언제든지 늘릴 수 있지만, 한 번 늘린 파티션 수는 절대로 줄일 수 없음을 명심하자
    - 초기에는 2 또는 4로 생성하는 것을 추천

- 세그먼트 : 브로커의 로컬 디스크에 저장되는 로그 파일 형태

<details>
<summary>세그먼트 확인</summary>
    <br>

- 카프카가 설치된 서버에 접속

```bash
cd /data/kafka-logs/
ls
cd peter-overview01-0
ls
xxd 000000000000000000.log
```

</details>

### 3.2) 카프카의 핵심 개념

- 분산 시스템 : 시스템 확장에 용이
- 페이지 캐시 : 높은 처리량
- 배치 전송 처리 : 네트워크 오버헤드 감소
- 압축 전송 : 성능 향상
    - 배치 전송과 함께 사용 시 효율 증가
    - 높은 압축률이 필요한 경우 gzip, zstd
    - 빠른 응답 속도가 필요한 경우 lz4, snappy
- 토픽, 파티션, 오프셋
    - 토픽 : 이메일 주소의 개념
    - 파티션 : 토픽의 병렬 처리를 위한 단위
    - 오프셋 : 파티션의 메시지가 저장되는 위치
- 고가용성 보장
    - 카프카에서 제공하는 리플리케이션 기능은 토픽 자체를 복제하는 것이 아니라 토픽의 파티션을 복제
    - 원본과 리플리케이션을 구분하기 위해 leader와 follower라고 부른다
- 주키퍼의 의존성
    - 주키퍼는 여러 대의 서버를 앙상블(클러스터)로 구성하고, 살아 있는 노드 수가 과반수 이상 유지된다면 지속적인 서비스가 가능한 구조
    - 주키퍼는 반드시 홀수로 구성해야 한다

### 3.3) 프로듀서의 기본 동작

![producer design](https://jashangoyal.files.wordpress.com/2019/03/producer.png?w=810)

- `ProducerRecord` : 카프카로 전송하기 위한 실제 데이터
    - 토픽과 밸류는 필수 입력
    - 파티션 값을 지정했다면 파티셔너는 아무런 동작도 하지 않게 된다
    - 파티션 값을 지정하지 않았다면 라운드 로빈 방식으로 동작
- 프로듀서가 카프카로 데이터를 배치로 전송하기 위해 `send()` 메서드 이후 레코드들을 파티션별로 잠시 모아둔다
- 프로듀서의 전송 방법은 크게 3가지 방식으로 나뉜다
    - 메세지를 보내고 확인하지 않기
        - `Future` 객체에서 `get()` 메서드를 호출하지 않음
        - 카프카 브로커에게 메세지를 전송한 후의 에러는 무시하지만, 전송 전에 에러가 발생하면 예외 처리 가능
    - 동기 전송
        - `Future` 객체에서 `get()` 메서드를 호출해 `RecordMetadata` 리턴 받음
        - 카프카로 메세지를 보내기 전과 보내는 동안 에러가 발생하면 예외가 발생
    - 비동기 전송
        - `org.apache.kafka.clients.producer.Callback` 구현체 클래스 생성 및 `onCompletion()` 메서드 오버라이드
        - `send()` 메서드에 레코드와 함께 콜백 객체를 전달
        - 빠른 전송이 가능하고, 메세지 전송이 실패해도 예외 처리 가능

### 3.4) 컨슈머의 기본 동작

- 메시지는 브로커의 로컬 디스크에 저장되어 있음
- 컨슈머는 반드시 컨슈머 그룹에 속하게 되며, 컨슈머 그룹은 각 파티션 리더에게 카프카 토픽에 저장된 메세지를 요청
- 파티션 수와 컨슈머 수는 일대일로 매핑되는 것이 이상적
- 컨슈머에서 메세지를 가져오는 방법은 크게 3가지 방식으로 나뉜다
    - 오토 커밋
        - `enable.auto.commit` 설정을 `true`로 적용
        - 많이 사용하는 방식
        - 오프셋을 주기적으로 커밋하므로 관리자가 오프셋을 따로 관리하지 않아도 된다
        - 반면, 컨슈머 종료 등이 빈번히 일어나면 일부 메세지를 못 가져오거나 중복으로 가져온다
    - 동기 가져오기
        - while 반복문 안에서 가져온 레코드 처리 이후 `consumer.commitSync()` 메서드 호출
        - 현재 배치를 통해 읽은 모든 메세지를 처리한 후, 추가 메세지를 폴링하기 전 현재의 오프셋을 동기 커밋
        - 속도는 느리지만, 메시지 손실은 거의 발생하지 않음
        - 메시지의 중복 이슈는 피할 수 없음
    - 비동기 가져오기
        - while 반복문 안에서 가져온 레코드 처리 이후 `consumer.commitAsync()` 메서드 호출
        - 현재 배치를 통해 읽은 모든 메세지를 처리한 후, 추가 메세지를 폴링하기 전 현재의 오프셋을 비동기 커밋
        - `commitAsync()`은 `commitSync()`과 달리 오프셋 커밋을 실패하더라도 재시도하지 않는다
            - 재시도로 인해 오프셋이 앞으로 당겨진만큼 메시지가 중복 처리된다
        - 비동기 커밋이 계속 실패하더라도 마지막의 비동기 커밋만 성공하면 되며, 콜백을 같이 사용해서 보완할 수 있다

# 4장. 카프카의 내부 동작 원리와 구현

### 4.1) 카프카 리플리케이션

<details>
<summary>간단한 코드 예제</summary>
<br>

- 카프카 토픽 생성
```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3
```

- 카프카 토픽 상세보기
```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test01 --describe
```

- 카프카 메세지 프로듀스
```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test01
```

- 카프카 세그먼트 파일 내용 확인
```bash
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
```
</details>

<br>

- 리플리케이션 동작 개요
    - N개의 리플리케이션이 있는 경우 N-1 까지의 브로커 장애가 발생해도 메시지 손실 없음
<br>
- 리더와 팔로워
    - 모든 읽기와 쓰기는 리더를 통해서만 가능하다
<br>
- 복제 유지와 커밋
    - 리더와 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있음
    - ISR에 속하지 않은 팔로워는 리더의 자격을 가질 수 없다
    - 팔로워는 지속적으로 리더를 따라가고, 리더는 ISR 내 모든 팔로워가 메세지를 받는지 감시한다
    - 팔로워가 지속적으로 따라오지 않으면 리더는 ISR 그룹에서 해당 팔로워를 추방
    <br>
    - ISR 내 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋 표시를 한다
    - 마지막 커밋 오프셋 위치는 하이워터마크(high water mark)라 부른다
    - 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다
    <br>
    - 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 replication-offset-checkpoint라는 파일에 마지막 커밋 오프셋 위치를 저장
    - 메세지가 프로듀스되고 커밋되면 1씩 증가됨을 확인할 수 있다

        <details>
        <summary>replication-offset-checkpoint 확인 코드</summary>

        ```bash
        cat /data/kafka-logs/replication-offset-checkpoint
        ```
        </details>

        <br>

- 딘계별 리플리케이션 동작
    - 서로의 통신을 최소화할 수 있도록 설계되어 리더의 부하를 줄였다
    <br>
    - 팔로워들의 fetch 요청으로 리더가 리플리케이션이 요청되었음은 알 수 있으나, 성공 유무는 알 수 없다
    - 래빗MQ의 트랜잭션 모드에서는 모든 미러가 메세지를 받았는지에 대한 ACK를 리더에게 리턴하므로 알 수 있지만, 카프카는 ACK 통신을 주고 받지 않는다
        - 성능을 높이기 위해 ACK 통신 제거
    
    <br>

    - 이후 새로운 메시지가 프로듀싱되어 팔로워들이 새로운 오프셋에 대한 리플리케이션을 요청하게 된다
    - 새로운 오프셋에 대한 리플리케이션 요청이 들어오면 리더는 이전 오프셋에 대한 동작은 성공했다고 인지하고 이전 오프셋에 커밋 표시를 한 후 하이워터마크를 증가시킨다
    - 이전 오프셋으로 요청하는 팔로워가 있으면 실패했음을 리더가 알 수 있다
    - 새로운 오프셋 메시지에 대한 요청을 받은 리더는 응답에 이전 오프셋이 커밋되었음을 함께 전달한다
    - 이전 오프셋이 커밋되었다는 응답을 받은 팔로워는 동일하게 커밋을 표시하고나서 새로운 메시지를 리플리케이션한다
    <br>
- 리더에포크와 복구
    - 리더에포크는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지
    - 컨트롤러에 관리되는 32비트의 숫자로 표현된다
    <br>
    - 리더와 팔로워 간 하이워터마크가 1 차이나기에 발생하는 문제를 해결한다
        1. 팔로워가 복구된 이후 자신의 하이워터마크보다 높은 메시지를 즉시 삭제해버리고, fetch 요청을 했을 때 리더가 다운된 경우
        2. 리더 팔로워 모두 다운되고 팔로워가 먼저 복구되어 리더로 승격되었고, 새로운 메시지를 받은 이후 기존의 리더가 복구된 경우
    - 1번 문제의 경우, 팔로워가 복구된 이후 자신의 하이워터마크보다 높은 메시지를 삭제하지 않고 리더에게 리더에포크를 요청하여 해결
    - 2번 문제의 경우, 새로운 리더는 자신이 팔로워일 때의 하이워터마크와 새로운 리더일 때의 라이워터마크를 알고 있으며, 리더에포크 요청이 오면 자신이 팔로워일 때의 하이워터마크를 보내서 이전 리더에만 있던 메시지를 없애고, 자신에게만 추가된 메시지를 리플리케이션한다

    <details>
    <summary>리더에포크 실습 코드</summary>

    ```bash
    # 토픽 생성
    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test02 --partitions 1 --replication-factor 3

    # 토픽 상세보기를 통해 리더가 어느 브로커인지 확인
    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test02 --describe

    # 리더 브로커 접속 이후 리더에포크 상태 확인
    cat /data/kafka-logs/peter-test02-0/leader-epoch-checkpoint

    # 메세지 프로듀스 -> 리더에포크 변화 없음
    /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka02:9092 --topic peter-test02

    # 리더 강제 종료 -> 이후 새로운 리더 브로커로 접속
    sudo systemctl stop kafka-server
    sudo systemctl status kafka-server

    # 새로운 리더의 리더에포크 상태 확인
    cat /data/kafka-logs/peter-test02-0/leader-epoch-checkpoint
    ```
    </details>

### 4.2) 컨트롤러

- 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하며, 파티션의 ISR 리스트 중에서 리더를 선출
    - ISR 리스트 정보는 가용성 보장을 위해 주키퍼에 저장
    - 브로커의 실패가 감지되면 즉시 ISR 리스트 중 하나를 새로운 파티션 리더로 선출하고, 주키퍼에 기록하며, 변경 사항을 모든 브로커에게 전달한다
- 클라이언트에 설정되어 있는 재시도 숫자만큼 재시도를 하게 되므로 새로운 리더 선출 작업이 빠르게 이뤄져야 한다

<details>
<summary>예기치 않은 상황에서의 리더 선출 과정</summary>

1. 파티션의 리더가 있는 브로커가 다운
2. 주키퍼는 브로커와 연결이 끊어진 후, 파티션의 ISR에서 변화를 감지
3. 컨트롤러는 주키퍼 워치를 통해 파티션 변화를 감지하고, 새로운 리더를 선출
4. 컨트롤러는 해당 파티션의 새로운 리더에 대한 정보를 주키퍼에 기록
5. 갱신된 정보는 활성화 상태인 모든 브로커에 전달
</details>
<details>
<summary>자연스러운 종료 상황에서의 리더 선출 과정</summary>

1. 관리자가 브로커 종료 명령어를 실행하고, `SIG_TERM` 신호가 브로커에 전달
2. `SIG_TERM` 신호를 받은 브로커는 컨트롤러에게 알림
3. 컨트롤러는 리더 선출 작업을 진행하고, 해당 정보를 주키퍼에 기록
4. 컨트롤러는 새로운 리더 정보를 다른 브로커에 전송
5. 컨트롤러는 종료 요청을 보낸 브로커에게 정상 종료한다는 응답을 보낸다
6. 응답을 받은 브로커는 캐시에 있는 내용을 디스크에 저장하고 종료한다
</details>
<br>

- 제어된 종료와 급작스러운 종료의 가장 큰 차이는 다운타임이다
    - 제어된 종료를 사용하면 카프카 내부적으로 파티션들의 다운타임을 최소화할 수 있다
    - 브로커가 종료되기 전에 리더 선출 작업을 진행하기 때문이다
    - 제어된 종료에서는 모든 로그를 디스크에 동기화한 후 종료되므로, 재시작할 때 로그 복구 시간이 짧다
    - `controlled.shutdown.enable = true` 설정을 통해 제어된 종료를 사용 가능하다

        <details>
        <summary>현재 브로커 설정 확인</summary>

        ```bash
        /usr/local/kafka/bin/kafka-configs.sh --bootstrap-server peter-kafka01:9092 --broker 1 --describe --all
        ```
        </details>

### 4.3) 로그(로그 세그먼트)

- 카프카의 토픽으로 들어오는 메시지는 세그먼트라는 파일에 저장된다
- 로그 세그먼트에는 내용 뿐만 아니라 메시지의 키, 밸류, 오프셋, 메시지 크기 같은 정보가 함께 저장되며, 브로커의 로컬 디스크에 보관된다
- 최대 크기는 1GB가 기본값으로 설정되어 있므여, 1GB보다 커지는 경우 기본적으로 롤링 전략을 사용
    - 1GB에 도달하면 해당 세그먼트 파일을 close하고, 새로운 로그 세그먼트를 생성하는 방식
- 로그 세그먼트를 관리하는 방법은 아래 두가지로 크게 나뉜다
    - 로그 세그먼트 삭제
    - 로그 컴팩션
<br>

- 로그 세그먼트 삭제
    - 브로커의 설정 파일인 `server.properties`에서 `log.cleanup.policy`가 `delete`로 명시되어야 한다
    - 로그 세그먼트 파일명 생성 규칙
        - 오프셋 시작 번호를 이용해 파일 이름 생성
    - `retention.ms` 기본 값은 7일
    - `retention.bytes` 옵션을 이용해 시간이 아닌 지정된 크기 기준으로 로그 세그먼트 삭제 가능

        <details>
        <summary>실습 코드</summary>

        ```bash
        # 실습용 토픽 생성
        /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test03 --partitions 1 --replication-factor 3

        # 메세지 전송
        /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03

        # 토픽의 처음부터 메시지 가져오기
        /usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --from-beginning

        # 메시지 삭제하기 옵션 추가
        # retention.ms = 0이란 로그 세그먼트 보관 시간이 해당 숫자보다 크면 세그먼트를 삭제한다는 명령
        /usr/local/kafka/bin/kafka-configs.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --add-config retention.ms=0 --alter

        # 설정 내용 확인
        /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --describe

        # 로그 세그먼트 상태 확인
        # 로그 세그먼트 삭제 작업의 일정 주기 기본값이 5분 주기이므로, 5분 뒤에 확인하면 00..000으로 시작하는 파일이 모두 삭제되고, 00..001으로 시작하는 파일이 새로 생성됨을 확인할 수 있다
        ls /data/kafka-logs/peter-test03-0/

        # retention.ms 옵션 삭제
        /usr/local/kafka/bin/kafka-configs.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --delete-config retention.ms --alter

        # 설정 내용 확인
        /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test03 --describe
        ```
        </details>
<br>

- 로그 컴팩션
    - 현재 활성화된 세그먼트를 제외하고 나머지 세그먼트를 대상으로 실행된다
    - 카프카에서 로그 세그먼트를 컴팩션하면 메시지의 키값을 기준으로 마지막의 데이터만 보관한다
    - 대표 예제는 `__consumer_offset` 토픽
        - 컨슈머 그룹의 정보를 저장하는 토픽
        - 해당 컨슈머 그룹이 어디까지 읽었는 지를 나타내는 오프셋 커밋 정보
        - 키(컨슈머 그룹명, 토픽명)와 밸류(오프셋 커밋 정보) 형태로 메시지가 저장
    - 로그 컴팩션 기능을 사용하려면, 프로듀서가 카프카로 메시지를 전송할 때 필수 값인 밸류 뿐만 아니라 키도 반드시 전송해야 한다
    <br>
    - 장점 : 빠른 장애 복구가 가능하다
        - 전체 로그를 복구하지 않고, 최신의 상태만 복구 가능하므로
        - 모든 토픽이 아닌 최종값만 필요한 워크로드에 적용하는 것이 바람직
    - 로그 컴팩션 작업 시에 브로커의 과도한 입출력 부하가 발생할 수 있으니 브로커의 리소스 모니터링과 병행하는 것을 권장