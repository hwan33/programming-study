# 실전 카프카 개발부터 운영까지

저자 : 고승범

# 1장. 카프카 개요

생략

# 2장. 카프카 환경 구성

<details>
<summary>키 페어 생성 및 권한 변경</summary>
<br>
    
- 키페어의 생성은 클라우드 콘솔에서 생성 가능
- 아래 명령어를 이용해 키 페어 권한 변경 가능

```bash
chmod 600 keypair.pem
```
</details>

<details>
<summary>키 페어 이용해서 퍼블릭 IP 주소 접근</summary>
    <br>

- 아래 두 명령어 모두 가능

```bash
ssh -i keypair.pem -l ec2-user 13.125.209.60
ssh -i keypair.pem ec2-user@13.125.209.60
```
</details>

<details>
<summary>/etc/hosts 수정을 통한 서버 IP와 호스트네임 매핑</summary>
    <br>

- /etc/hosts 파일 예제
    - ip는 사설 ip
    - 모든 인스턴스에 설정

> 172.31.3.209 peter-ansible01.foo.bar peter-ansible01
> ... // 생략

- ping test 명령어

```bash
ping -c 2 peter-zk01.foo.bar
```
</details>

<details>
<summary>앤서블 설치 및 기타 환경 설정</summary>
    <br>

- 앤서블 설치
```bash
sudo amazon-linux-extras install -y ansible2
```

- git 설치 및 책에서 제공하는 git clone

```bash
sudo yum install -y git
git clone https://github.com/onlybooks/kafka2
```

- 키 페어 인스턴스에 복사
    - 로컬 terminal에서

```bash
scp -i keypair.pem keypair.pem ec2-user@13.125.20.117:~
```

- 권한 변경 및 키 등록

```bash
chmod 600 keypair.pem
ssh-agent bash
ssh-add keypair.pem
```

</details>

<details>
<summary>ssh 공개 키를 생성해 사용하는 방식</summary>
    <br>

- 키를 메모리에 저장해두고 사용하는 방식은 배포 서버 재접속때 설정이 초기화되는 문제 발생
- 아래와 같은 방식으로 ssh 공개 키를 생성해 사용하는 방식도 존재한다

1. 배포 서버에서 공개 키 생성
2. 공개 키 내용을 접속하고자 하는 서버에 복사
3. 배포 서버에서 다른 서버로 비밀번호 없이 접속

```bash
ssh-keygen // 이후 무한 엔터
cat /home/ec2-user/.ssh/id_rsa.pub // 확인 후 복사
```

- 이후 각각의 서버에 로그인하여 아래 명령어 실행

```bash
vi /home/ec2-user/.ssh/authorized_keys
chmod 600 .ssh/authorized_keys
```

</details>

<details>
<summary>주키퍼 설치</summary>
    <br>

- 앤서블 명령어인 ansible-playbook을 써서 hosts 파일에 지정된 zookeeper 서버에 모두 주키퍼를 설치

```bash
cd kafka2/chapter2/ansible_playbook
ansible-playbook -i hosts zookeeper.yml
```

- 각각의 주키퍼 서버에 접근해 제대로 실행되는지 확인

```bash
sudo systemctl status zookeeper-server
```

</details>

<details>
<summary>카프카 설치</summary>
    <br>

- 앤서블 명령어인 ansible-playbook을 써서 hosts 파일에 지정된 kafka 서버에 모두 카프카를 설치

```bash
cd kafka2/chapter2/ansible_playbook
ansible-playbook -i hosts kafka.yml
```

- 각각의 카프카 서버에 접근해 제대로 실행되는지 확인

```bash
sudo systemctl status kafka-server
```

</details>

<details>
<summary>토픽 생성</summary>
    <br>

- 카프카가 설치된 서버에 접속한 후 카프카에서 제공하는 도구 중 kafka-topics.sh 명령어를 이용해 토픽 생성

```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-overview01 --partitions 1 --replication-factor 3
```

</details>

<details>
<summary>컨슈머, 프로듀서 실행 및 메세지 전달</summary>
    <br>

- 컨슈머 실행

```bash
/usr/local/kafka/bin/kafka-console-consumer.sh --bootstrap-server peter-kafka01:9092 --topic peter-overview01
```

- 프로듀서 실행

```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-overview01
```

- 메세지 전송
    - 프로듀서 실행 이후 명령 프롬포트가 `>`로 변경되면 메세지 입력
</details>

# 3장. 카프카 기본 개념과 구조

### 3.1) 카프카 기초 다지기

- 리플리케이션 : 각 메세지를 여러 개로 복제해서 카프카 클러스터 내 브로커들에 분산시키는 동작
    - 테스트나 개발 환경 : replication factor 1
    - 운영 환경 (로그성 메세지로서 약간의 유실 허용) : replication factor 2
    - 운영 환경 (유실 허용하지 않음) : replication factor 3

- 파티션 : 하나의 토픽이 한 번에 처리할 수 있는 한계를 높이기 위해 토픽 하나를 여러 개로 나눠 병렬 처리가 가능하게 만든 것
    - 파티션 수는 초기 생성 후 언제든지 늘릴 수 있지만, 한 번 늘린 파티션 수는 절대로 줄일 수 없음을 명심하자
    - 초기에는 2 또는 4로 생성하는 것을 추천

- 세그먼트 : 브로커의 로컬 디스크에 저장되는 로그 파일 형태

<details>
<summary>세그먼트 확인</summary>
    <br>

- 카프카가 설치된 서버에 접속

```bash
cd /data/kafka-logs/
ls
cd peter-overview01-0
ls
xxd 000000000000000000.log
```

</details>

### 3.2) 카프카의 핵심 개념

- 분산 시스템 : 시스템 확장에 용이
- 페이지 캐시 : 높은 처리량
- 배치 전송 처리 : 네트워크 오버헤드 감소
- 압축 전송 : 성능 향상
    - 배치 전송과 함께 사용 시 효율 증가
    - 높은 압축률이 필요한 경우 gzip, zstd
    - 빠른 응답 속도가 필요한 경우 lz4, snappy
- 토픽, 파티션, 오프셋
    - 토픽 : 이메일 주소의 개념
    - 파티션 : 토픽의 병렬 처리를 위한 단위
    - 오프셋 : 파티션의 메시지가 저장되는 위치
- 고가용성 보장
    - 카프카에서 제공하는 리플리케이션 기능은 토픽 자체를 복제하는 것이 아니라 토픽의 파티션을 복제
    - 원본과 리플리케이션을 구분하기 위해 leader와 follower라고 부른다
- 주키퍼의 의존성
    - 주키퍼는 여러 대의 서버를 앙상블(클러스터)로 구성하고, 살아 있는 노드 수가 과반수 이상 유지된다면 지속적인 서비스가 가능한 구조
    - 주키퍼는 반드시 홀수로 구성해야 한다

### 3.3) 프로듀서의 기본 동작

![producer design](https://jashangoyal.files.wordpress.com/2019/03/producer.png?w=810)

- `ProducerRecord` : 카프카로 전송하기 위한 실제 데이터
    - 토픽과 밸류는 필수 입력
    - 파티션 값을 지정했다면 파티셔너는 아무런 동작도 하지 않게 된다
    - 파티션 값을 지정하지 않았다면 라운드 로빈 방식으로 동작
- 프로듀서가 카프카로 데이터를 배치로 전송하기 위해 `send()` 메서드 이후 레코드들을 파티션별로 잠시 모아둔다
- 프로듀서의 전송 방법은 크게 3가지 방식으로 나뉜다
    - 메세지를 보내고 확인하지 않기
        - `Future` 객체에서 `get()` 메서드를 호출하지 않음
        - 카프카 브로커에게 메세지를 전송한 후의 에러는 무시하지만, 전송 전에 에러가 발생하면 예외 처리 가능
    - 동기 전송
        - `Future` 객체에서 `get()` 메서드를 호출해 `RecordMetadata` 리턴 받음
        - 카프카로 메세지를 보내기 전과 보내는 동안 에러가 발생하면 예외가 발생
    - 비동기 전송
        - `org.apache.kafka.clients.producer.Callback` 구현체 클래스 생성 및 `onCompletion()` 메서드 오버라이드
        - `send()` 메서드에 레코드와 함께 콜백 객체를 전달
        - 빠른 전송이 가능하고, 메세지 전송이 실패해도 예외 처리 가능

### 3.4) 컨슈머의 기본 동작

- 메시지는 브로커의 로컬 디스크에 저장되어 있음
- 컨슈머는 반드시 컨슈머 그룹에 속하게 되며, 컨슈머 그룹은 각 파티션 리더에게 카프카 토픽에 저장된 메세지를 요청
- 파티션 수와 컨슈머 수는 일대일로 매핑되는 것이 이상적
- 컨슈머에서 메세지를 가져오는 방법은 크게 3가지 방식으로 나뉜다
    - 오토 커밋
        - `enable.auto.commit` 설정을 `true`로 적용
        - 많이 사용하는 방식
        - 오프셋을 주기적으로 커밋하므로 관리자가 오프셋을 따로 관리하지 않아도 된다
        - 반면, 컨슈머 종료 등이 빈번히 일어나면 일부 메세지를 못 가져오거나 중복으로 가져온다
    - 동기 가져오기
        - while 반복문 안에서 가져온 레코드 처리 이후 `consumer.commitSync()` 메서드 호출
        - 현재 배치를 통해 읽은 모든 메세지를 처리한 후, 추가 메세지를 폴링하기 전 현재의 오프셋을 동기 커밋
        - 속도는 느리지만, 메시지 손실은 거의 발생하지 않음
        - 메시지의 중복 이슈는 피할 수 없음
    - 비동기 가져오기
        - while 반복문 안에서 가져온 레코드 처리 이후 `consumer.commitAsync()` 메서드 호출
        - 현재 배치를 통해 읽은 모든 메세지를 처리한 후, 추가 메세지를 폴링하기 전 현재의 오프셋을 비동기 커밋
        - `commitAsync()`은 `commitSync()`과 달리 오프셋 커밋을 실패하더라도 재시도하지 않는다
            - 재시도로 인해 오프셋이 앞으로 당겨진만큼 메시지가 중복 처리된다
        - 비동기 커밋이 계속 실패하더라도 마지막의 비동기 커밋만 성공하면 되며, 콜백을 같이 사용해서 보완할 수 있다

# 4장. 카프카의 내부 동작 원리와 구현

### 4.1) 카프카 리플리케이션

<details>
<summary>간단한 코드 예제</summary>
<br>

- 카프카 토픽 생성
```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test01 --partitions 1 --replication-factor 3
```

- 카프카 토픽 상세보기
```bash
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test01 --describe
```

- 카프카 메세지 프로듀스
```bash
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka01:9092 --topic peter-test01
```

- 카프카 세그먼트 파일 내용 확인
```bash
/usr/local/kafka/bin/kafka-dump-log.sh --print-data-log --files /data/kafka-logs/peter-test01-0/00000000000000000000.log
```
</details>

<br>

- 리플리케이션 동작 개요
    - N개의 리플리케이션이 있는 경우 N-1 까지의 브로커 장애가 발생해도 메시지 손실 없음
<br>
- 리더와 팔로워
    - 모든 읽기와 쓰기는 리더를 통해서만 가능하다
<br>
- 복제 유지와 커밋
    - 리더와 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있음
    - ISR에 속하지 않은 팔로워는 리더의 자격을 가질 수 없다
    - 팔로워는 지속적으로 리더를 따라가고, 리더는 ISR 내 모든 팔로워가 메세지를 받는지 감시한다
    - 팔로워가 지속적으로 따라오지 않으면 리더는 ISR 그룹에서 해당 팔로워를 추방
    <br>
    - ISR 내 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋 표시를 한다
    - 마지막 커밋 오프셋 위치는 하이워터마크(high water mark)라 부른다
    - 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있다
    <br>
    - 모든 브로커는 재시작될 때, 커밋된 메시지를 유지하기 위해 로컬 디스크의 replication-offset-checkpoint라는 파일에 마지막 커밋 오프셋 위치를 저장
    - 메세지가 프로듀스되고 커밋되면 1씩 증가됨을 확인할 수 있다

        <details>
        <summary>replication-offset-checkpoint 확인 코드</summary>

        ```bash
        cat /data/kafka-logs/replication-offset-checkpoint
        ```
        </details>

        <br>

- 딘계별 리플리케이션 동작
    - 서로의 통신을 최소화할 수 있도록 설계되어 리더의 부하를 줄였다
    <br>
    - 팔로워들의 fetch 요청으로 리더가 리플리케이션이 요청되었음은 알 수 있으나, 성공 유무는 알 수 없다
    - 래빗MQ의 트랜잭션 모드에서는 모든 미러가 메세지를 받았는지에 대한 ACK를 리더에게 리턴하므로 알 수 있지만, 카프카는 ACK 통신을 주고 받지 않는다
        - 성능을 높이기 위해 ACK 통신 제거
    
    <br>

    - 이후 새로운 메시지가 프로듀싱되어 팔로워들이 새로운 오프셋에 대한 리플리케이션을 요청하게 된다
    - 새로운 오프셋에 대한 리플리케이션 요청이 들어오면 리더는 이전 오프셋에 대한 동작은 성공했다고 인지하고 이전 오프셋에 커밋 표시를 한 후 하이워터마크를 증가시킨다
    - 이전 오프셋으로 요청하는 팔로워가 있으면 실패했음을 리더가 알 수 있다
    - 새로운 오프셋 메시지에 대한 요청을 받은 리더는 응답에 이전 오프셋이 커밋되었음을 함께 전달한다
    - 이전 오프셋이 커밋되었다는 응답을 받은 팔로워는 동일하게 커밋을 표시하고나서 새로운 메시지를 리플리케이션한다
    <br>
- 리더에포크와 복구
    - 리더에포크는 카프카의 파티션들이 복구 동작을 할 때 메시지의 일관성을 유지
    - 컨트롤러에 관리되는 32비트의 숫자로 표현된다
    <br>
    - 리더와 팔로워 간 하이워터마크가 1 차이나기에 발생하는 문제를 해결한다
        1. 팔로워가 복구된 이후 자신의 하이워터마크보다 높은 메시지를 즉시 삭제해버리고, fetch 요청을 했을 때 리더가 다운된 경우
        2. 리더 팔로워 모두 다운되고 팔로워가 먼저 복구되어 리더로 승격되었고, 새로운 메시지를 받은 이후 기존의 리더가 복구된 경우
    - 1번 문제의 경우, 팔로워가 복구된 이후 자신의 하이워터마크보다 높은 메시지를 삭제하지 않고 리더에게 리더에포크를 요청하여 해결
    - 2번 문제의 경우, 새로운 리더는 자신이 팔로워일 때의 하이워터마크와 새로운 리더일 때의 라이워터마크를 알고 있으며, 리더에포크 요청이 오면 자신이 팔로워일 때의 하이워터마크를 보내서 이전 리더에만 있던 메시지를 없애고, 자신에게만 추가된 메시지를 리플리케이션한다

    <details>
    <summary>리더에포크 실습 코드</summary>

    ```bash
    # 토픽 생성
    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --create --topic peter-test02 --partitions 1 --replication-factor 3

    # 토픽 상세보기를 통해 리더가 어느 브로커인지 확인
    /usr/local/kafka/bin/kafka-topics.sh --bootstrap-server peter-kafka01:9092 --topic peter-test02 --describe

    # 리더 브로커 접속 이후 리더에포크 상태 확인
    cat /data/kafka-logs/peter-test02-0/leader-epoch-checkpoint

    # 메세지 프로듀스 -> 리더에포크 변화 없음
    /usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server peter-kafka02:9092 --topic peter-test02

    # 리더 강제 종료 -> 이후 새로운 리더 브로커로 접속
    sudo systemctl stop kafka-server
    sudo systemctl status kafka-server

    # 새로운 리더의 리더에포크 상태 확인
    cat /data/kafka-logs/peter-test02-0/leader-epoch-checkpoint
    ```
    </details>
